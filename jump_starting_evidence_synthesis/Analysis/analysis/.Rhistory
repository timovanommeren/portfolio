plot.background  = element_rect(fill = "white", color = NA),
legend.background = element_rect(fill = "white", color = NA),
legend.key = element_rect(fill = "white", color = NA),
plot.margin = margin(10, 20, 10, 10)
)
})
# 2) Write to a multi-page PDF with 4 plots per page
pdf("..\\results\\papers_found_histograms.pdf", width = 11, height = 8.5, onefile = TRUE)
for (i in seq(1, length(plots), by = 4)) {
page_plots <- plots[i:min(i + 3, length(plots))]
print(wrap_plots(page_plots, ncol = 2, nrow = 2))
}
??wrap_plots
# 1) Build a list of plots (one per dataset)
dataset_names <- unique(simulation$dataset)
plots <- lapply(dataset_names, function(dataset_name) {
ggplot(simulation %>% filter(dataset == dataset_name),
aes(x = papers_found / n_trials, fill = condition)) +
geom_histogram(position = "dodge", bins = 30, alpha = 0.7) +
labs(
title = paste("N relevant records found by condition -", dataset_name),
x = "Relevant records found (X) divided by number of trials (n <=100)",
y = "Count"
) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, face = "bold"),
panel.background = element_rect(fill = "white", color = NA),
plot.background  = element_rect(fill = "white", color = NA),
legend.background = element_rect(fill = "white", color = NA),
legend.key = element_rect(fill = "white", color = NA),
plot.margin = margin(10, 20, 10, 10)
)
})
# 2) Write to a multi-page PDF with 4 plots per page
pdf("..\\results\\papers_found_histograms.pdf", width = 11, height = 8.5, onefile = TRUE)
for (i in seq(1, length(plots), by = 4)) {
page_plots <- plots[i:min(i + 3, length(plots))]
print(patchwork::wrap_plots(page_plots, ncol = 2, nrow = 2))
}
dev.off()
# import simulation results and metadata (specify paths relative to project root)
raw <- load_simulation_data(data = "simulation_results/correct_trials_w_ivs/all_simulation_results.csv",
metadata = "Analysis/percentage_relevant.csv")
# (1) transform simulation data to wide
# (2) add the number of records, percentage of relevant records and dataset topic(s) from metadata to simulation data (these will serve as group-level predictors in multilevel model)
# (4) rescale n_abstracts (divide by 100) and length_abstracts (divide by 1000)
# (5) truncate topic names to first topic only
processed <- prepare_data(raw$simulation, raw$meta)
processed$simulation <- processed$simulation %>%
filter(run <= 270)
processed$simulation <- processed$simulation %>%
filter(dataset != "Moran_2021")
# two-part coding of llm-specific predictors (implicit dummy variable creation through zero imputation, see reproduction_Dziak_Henry.Rmd for verification of method)
processed$simulation <- two_part_coding(processed$simulation)
# assign to variables for easier access
simulation_long <- raw$simulation
simulation      <- processed$simulation
metadata        <- processed$meta
unique_value_table <- simulation %>%
group_by(dataset, condition) %>%
summarise(
n_unique_values = n_distinct(papers_found),
.groups = "drop"
)
unique_value_table
unique_value_table %>%
filter(n_unique_values <= 2)
library(here)
here::i_am("Analysis/analysis/main_analysis.Rmd")
source(here("Analysis/R/00_packages.R"))
source(here("Analysis/R/01_load_data.R"))
source(here("Analysis/R/02_prepare_data.R"))
source(here("Analysis/R/03_descriptives.R"))
# source(here("R/04_models.R"))
# source(here("R/05_plots.R"))
# import simulation results and metadata (specify paths relative to project root)
raw <- load_simulation_data(data = "simulation_results/correct_trials_w_ivs/all_simulation_results.csv",
metadata = "Analysis/percentage_relevant.csv")
# (1) transform simulation data to wide
# (2) add the number of records, percentage of relevant records and dataset topic(s) from metadata to simulation data (these will serve as group-level predictors in multilevel model)
# (4) rescale n_abstracts (divide by 100) and length_abstracts (divide by 1000)
# (5) truncate topic names to first topic only
processed <- prepare_data(raw$simulation, raw$meta)
processed$simulation <- processed$simulation %>%
filter(run <= 270)
processed$simulation <- processed$simulation %>%
filter(dataset != "Moran_2021")
# two-part coding of llm-specific predictors (implicit dummy variable creation through zero imputation, see reproduction_Dziak_Henry.Rmd for verification of method)
processed$simulation <- two_part_coding(processed$simulation)
# assign to variables for easier access
simulation_long <- raw$simulation
simulation      <- processed$simulation
metadata        <- processed$meta
descriptive_barchart(simulation, metadata, papers_found, y_axis = 100)
#scatterplot of the relationship between meand and (binomial) variance per dataset
ggplot(simulation %>%
#filter(condition != "no_initialisation") %>%
group_by(dataset, condition) %>%
summarise(
p_hat = mean(papers_found / n_trials),
var_obs = var(papers_found / n_trials),
var_binom_expected = p_hat * (1 - p_hat) * mean(1 / n_trials),
.groups = "drop"
),
aes(x = p_hat, y = var_obs, color = condition)) +
geom_point() +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") + # y=x line
geom_line(aes(y = var_binom_expected), linetype = "dotted", linewidth = 1) + # binomial variance line
labs(
title = "Mean vs observed variance of papers found per dataset and condition",
x = "Mean proportion of papers found",
y = "Observed variance of proportion of papers found"
) +
theme_minimal()
p <- ggplot(simulation, aes(x = papers_found / n_trials, fill = condition)) +
geom_histogram(position = "dodge", bins = 30, alpha = 0.7) +
labs(
title = "Distribution of number of relevant records found by condition",
x = "Relevant records found (X) divided by number of trials (n <=100)",
y = "Count"
) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, face = "bold"),
panel.background = element_rect(fill = "white", color = NA),
plot.background  = element_rect(fill = "white", color = NA),
legend.background = element_rect(fill = "white", color = NA),
legend.key = element_rect(fill = "white", color = NA),
plot.margin = margin(10, 20, 10, 10)
)
# Show in R *with* title
p
# Export *without* title
ggsave(
filename = here::here("Report/results/papers_found_histogram.png"),
plot = p + labs(title = NULL),     # or + theme(plot.title = element_blank())
width = 10,
height = 6,
dpi = 300
)
# 1) Build a list of plots (one per dataset)
dataset_names <- unique(simulation$dataset)
plots <- lapply(dataset_names, function(dataset_name) {
ggplot(simulation %>% filter(dataset == dataset_name),
aes(x = papers_found / n_trials, fill = condition)) +
geom_histogram(position = "dodge", bins = 30, alpha = 0.7) +
labs(
title = paste("N relevant records found by condition -", dataset_name),
x = "Relevant records found (X) divided by number of trials (n <=100)",
y = "Count"
) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, face = "bold"),
panel.background = element_rect(fill = "white", color = NA),
plot.background  = element_rect(fill = "white", color = NA),
legend.background = element_rect(fill = "white", color = NA),
legend.key = element_rect(fill = "white", color = NA),
plot.margin = margin(10, 20, 10, 10)
)
})
# 2) Write to a multi-page PDF with 4 plots per page
pdf("..\\results\\papers_found_histograms.pdf", width = 11, height = 8.5, onefile = TRUE)
# 1) Build a list of plots (one per dataset)
dataset_names <- unique(simulation$dataset)
plots <- lapply(dataset_names, function(dataset_name) {
ggplot(simulation %>% filter(dataset == dataset_name),
aes(x = papers_found / n_trials, fill = condition)) +
geom_histogram(position = "dodge", bins = 30, alpha = 0.7) +
labs(
title = paste("N relevant records found by condition -", dataset_name),
x = "Relevant records found (X) divided by number of trials (n <=100)",
y = "Count"
) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, face = "bold"),
panel.background = element_rect(fill = "white", color = NA),
plot.background  = element_rect(fill = "white", color = NA),
legend.background = element_rect(fill = "white", color = NA),
legend.key = element_rect(fill = "white", color = NA),
plot.margin = margin(10, 20, 10, 10)
)
})
# 2) Write to a multi-page PDF with 4 plots per page
pdf("..\\results\\papers_found_histograms.pdf", width = 11, height = 8.5, onefile = TRUE)
for (i in seq(1, length(plots), by = 4)) {
page_plots <- plots[i:min(i + 3, length(plots))]
print(patchwork::wrap_plots(page_plots, ncol = 2, nrow = 2))
}
dev.off()
unique_value_table <- simulation %>%
group_by(dataset, condition) %>%
summarise(
n_unique_values = n_distinct(papers_found),
.groups = "drop"
)
unique_value_table
unique_value_table %>%
filter(n_unique_values <= 2)
unique_value_table %>%
filter(n_unique_values <= 2)
#check the assumption of homogeneity of variance across conditions
ggplot(simulation, aes(x = condition, y = papers_found / n_trials)) +
geom_boxplot() +
geom_jitter(width = 0.1, alpha = .3)
#calculate Levene's test for homogeneity of variance
car::leveneTest(papers_found / n_trials ~ condition, data = simulation) # not significant, so homogeneity of variance assumption met
#set reference category for regression model
simulation <- simulation %>% mutate(condition = relevel(factor(condition), ref = "llm"))
bin_model0 <- lme4::glmer(cbind(papers_found, n_trials - papers_found) ~ 1 + (1|dataset),
family = binomial(link = "logit"),
data = simulation,
control = lme4::glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun=2e5)))
performance::icc(bin_model0) # adjusted means adjusted for fixed effects
bin_model1 <- lme4::glmer(cbind(papers_found, n_trials - papers_found) ~ condition + (1|dataset),
family = binomial(link = "logit"),
data = simulation,
control = lme4::glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun=2e5)))
summary(bin_model1)
anova(bin_model1, bin_model0)
performance::r2_nakagawa(bin_model1)
simulationOutput_bin_model1 <- simulateResiduals(fittedModel = bin_model1, plot = F, n = 1000, re.form = NULL)
library(DHARMa)
simulationOutput_bin_model1 <- simulateResiduals(fittedModel = bin_model1, plot = F, n = 1000, re.form = NULL)
DHARMa::testOutliers(simulationOutput_bin_model1, type = "bootstrap")
plotQQunif(simulationOutput_bin_model1)
plotResiduals(simulationOutput_bin_model1, form = simulation$condition)
library(here)
here::i_am("Analysis/analysis/main_analysis.Rmd")
source(here("Analysis/R/00_packages.R"))
source(here("Analysis/R/01_load_data.R"))
source(here("Analysis/R/02_prepare_data.R"))
source(here("Analysis/R/03_descriptives.R"))
# source(here("R/04_models.R"))
# source(here("R/05_plots.R"))
# import simulation results and metadata (specify paths relative to project root)
raw <- load_simulation_data(data = "simulation_results/correct_trials_w_ivs/all_simulation_results.csv",
metadata = "Analysis/percentage_relevant.csv")
# (1) transform simulation data to wide
# (2) add the number of records, percentage of relevant records and dataset topic(s) from metadata to simulation data (these will serve as group-level predictors in multilevel model)
# (4) rescale n_abstracts (divide by 100) and length_abstracts (divide by 1000)
# (5) truncate topic names to first topic only
processed <- prepare_data(raw$simulation, raw$meta)
processed$simulation <- processed$simulation %>%
filter(run <= 270)
processed$simulation <- processed$simulation %>%
filter(dataset != "Moran_2021")
# two-part coding of llm-specific predictors (implicit dummy variable creation through zero imputation, see reproduction_Dziak_Henry.Rmd for verification of method)
processed$simulation <- two_part_coding(processed$simulation)
# assign to variables for easier access
simulation_long <- raw$simulation
simulation      <- processed$simulation
metadata        <- processed$meta
descriptive_barchart(simulation, metadata, papers_found, y_axis = 100)
#scatterplot of the relationship between meand and (binomial) variance per dataset
ggplot(simulation %>%
#filter(condition != "no_initialisation") %>%
group_by(dataset, condition) %>%
summarise(
p_hat = mean(papers_found / n_trials),
var_obs = var(papers_found / n_trials),
var_binom_expected = p_hat * (1 - p_hat) * mean(1 / n_trials),
.groups = "drop"
),
aes(x = p_hat, y = var_obs, color = condition)) +
geom_point() +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") + # y=x line
geom_line(aes(y = var_binom_expected), linetype = "dotted", linewidth = 1) + # binomial variance line
labs(
title = "Mean vs observed variance of papers found per dataset and condition",
x = "Mean proportion of papers found",
y = "Observed variance of proportion of papers found"
) +
theme_minimal()
p <- ggplot(simulation, aes(x = papers_found / n_trials, fill = condition)) +
geom_histogram(position = "dodge", bins = 30, alpha = 0.7) +
labs(
title = "Distribution of number of relevant records found by condition",
x = "Relevant records found (X) divided by number of trials (n <=100)",
y = "Count"
) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, face = "bold"),
panel.background = element_rect(fill = "white", color = NA),
plot.background  = element_rect(fill = "white", color = NA),
legend.background = element_rect(fill = "white", color = NA),
legend.key = element_rect(fill = "white", color = NA),
plot.margin = margin(10, 20, 10, 10)
)
# Show in R *with* title
p
# Export *without* title
ggsave(
filename = here::here("Report/results/papers_found_histogram.png"),
plot = p + labs(title = NULL),     # or + theme(plot.title = element_blank())
width = 10,
height = 6,
dpi = 300
)
# 1) Build a list of plots (one per dataset)
dataset_names <- unique(simulation$dataset)
plots <- lapply(dataset_names, function(dataset_name) {
ggplot(simulation %>% filter(dataset == dataset_name),
aes(x = papers_found / n_trials, fill = condition)) +
geom_histogram(position = "dodge", bins = 30, alpha = 0.7) +
labs(
title = paste("N relevant records found by condition -", dataset_name),
x = "Relevant records found (X) divided by number of trials (n <=100)",
y = "Count"
) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, face = "bold"),
panel.background = element_rect(fill = "white", color = NA),
plot.background  = element_rect(fill = "white", color = NA),
legend.background = element_rect(fill = "white", color = NA),
legend.key = element_rect(fill = "white", color = NA),
plot.margin = margin(10, 20, 10, 10)
)
})
# 2) Write to a multi-page PDF with 4 plots per page
pdf("..\\results\\papers_found_histograms.pdf", width = 11, height = 8.5, onefile = TRUE)
for (i in seq(1, length(plots), by = 4)) {
page_plots <- plots[i:min(i + 3, length(plots))]
print(patchwork::wrap_plots(page_plots, ncol = 2, nrow = 2))
}
dev.off()
unique_value_table <- simulation %>%
group_by(dataset, condition) %>%
summarise(
n_unique_values = n_distinct(papers_found),
.groups = "drop"
)
unique_value_table
unique_value_table %>%
filter(n_unique_values <= 2)
#check the assumption of homogeneity of variance across conditions
ggplot(simulation, aes(x = condition, y = papers_found / n_trials)) +
geom_boxplot() +
geom_jitter(width = 0.1, alpha = .3)
#calculate Levene's test for homogeneity of variance
car::leveneTest(papers_found / n_trials ~ condition, data = simulation) # not significant, so homogeneity of variance assumption met
#set reference category for regression model
simulation <- simulation %>% mutate(condition = relevel(factor(condition), ref = "llm"))
bin_model0 <- lme4::glmer(cbind(papers_found, n_trials - papers_found) ~ 1 + (1|dataset),
family = binomial(link = "logit"),
data = simulation,
control = lme4::glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun=2e5)))
performance::icc(bin_model0) # adjusted means adjusted for fixed effects
bin_model1 <- lme4::glmer(cbind(papers_found, n_trials - papers_found) ~ condition + (1|dataset),
family = binomial(link = "logit"),
data = simulation,
control = lme4::glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun=2e5)))
summary(bin_model1)
anova(bin_model1, bin_model0)
performance::r2_nakagawa(bin_model1)
simulationOutput_bin_model1 <- simulateResiduals(fittedModel = bin_model1, plot = F, n = 1000, re.form = NULL)
DHARMa::testOutliers(simulationOutput_bin_model1, type = "bootstrap")
plotQQunif(simulationOutput_bin_model1)
plotResiduals(simulationOutput_bin_model1, form = simulation$condition)
betabin_model0 <- glmmTMB::glmmTMB(
cbind(papers_found, n_trials - papers_found) ~ 1 + (1|dataset),
family = glmmTMB::betabinomial(link = "logit"),
dispformula = ~ 1,
data = simulation
)
summary(betabin_model0)
performance::icc(betabin_model0)
betabin_model1 <- glmmTMB::glmmTMB(
cbind(papers_found, n_trials - papers_found) ~ condition + (1|dataset),
family = glmmTMB::betabinomial(link = "logit"),
dispformula = ~ 1,
data = simulation
)
summary(betabin_model1)
performance::r2_nakagawa(betabin_model1)
# #conduct LRT between betabin_model0 and bin_model0 manually by extracting chi-squared
# Chi_sq = 2 * (logLik(betabin_model1) - logLik(bin_model1))
# p_value = pchisq(Chi_sq, df = attr(logLik(betabin_model1), "df") - attr(logLik(bin_model1), "df"), lower.tail = FALSE)
# print(p_value)
#performance::icc(betabin_model0)
simulationOutput_betabin_model0 <- simulateResiduals(fittedModel = betabin_model0, plot = F, n = 1000, re.form = NULL)
DHARMa::testOutliers(simulationOutput_betabin_model0, type = "bootstrap")
plotQQunif(simulationOutput_betabin_model0)
plotResiduals(simulationOutput_betabin_model0, form = simulation$condition)
plotResiduals(simulationOutput_betabin_model0, simulation$dataset)
testZeroInflation(simulationOutput_betabin_model0)
betabin_model2 <- glmmTMB::glmmTMB(
cbind(papers_found, n_trials - papers_found) ~ condition + (1|dataset),
family = glmmTMB::betabinomial(link = "logit"),
dispformula = ~ 1,
ziformula = ~1,
data = simulation
)
summary(betabin_model2)
anova(betabin_model2, betabin_model1)
performance::r2_nakagawa(betabin_model2)
simulationOutput_betabin_model1 <- simulateResiduals(fittedModel = betabin_model1, plot = F, n = 1000, re.form = NULL)
DHARMa::testOutliers(simulationOutput_betabin_model1, type = "bootstrap")
plotQQunif(simulationOutput_betabin_model1)
plotResiduals(simulationOutput_betabin_model1, form = simulation$condition)
plotResiduals(simulationOutput_betabin_model1, simulation$dataset)
testZeroInflation(simulationOutput_betabin_model1)
nat_coefs <- summary(betabin_model1)$coefficients$cond
plogis(nat_coefs[1,1]) # the probability of finding a relevant record in the 'llm' condition
plogis(nat_coefs[1,1] + nat_coefs[2,1]) # the probability for the 'cold start' condition
plogis(nat_coefs[1,1] + nat_coefs[3,1]) # the probability for the 'criteria' condition
plogis(nat_coefs[1,1] + nat_coefs[4,1]) # the probability for the 'random' condition
exp(summary(betabin_model1)$coefficients$cond[c(2,3,4),1]) # odds ratios for other conditions compared to 'llm'
betabin_model3 <- glmmTMB::glmmTMB(
cbind(papers_found, n_trials - papers_found) ~ condition + n_abstracts_llm + length_abstracts_llm + llm_temperature_llm + (1|dataset),
family = glmmTMB::betabinomial(link = "logit"),
dispformula = ~ 1,
ziformula = ~ 1,
data = simulation
)
summary(betabin_model3)
anova(betabin_model3, betabin_model2)
nat_coefs <- summary(betabin_model2)$coefficients$cond
plogis(nat_coefs[1,1]) # the probability of finding a relevant record in the 'llm' condition
plogis(nat_coefs[1,1] + nat_coefs[2,1]) # the probability for the 'cold start' condition
plogis(nat_coefs[1,1] + nat_coefs[3,1]) # the probability for the 'criteria' condition
plogis(nat_coefs[1,1] + nat_coefs[4,1]) # the probability for the 'random' condition
exp(summary(betabin_model2)$coefficients$cond[c(2,3,4),1]) # odds ratios for other conditions compared to 'llm'
betabin_model4 <- glmmTMB::glmmTMB(
cbind(papers_found, n_trials - papers_found) ~
condition +
n_abstracts_llm +
(1|dataset),
family = glmmTMB::betabinomial(link = "logit"),
dispformula = ~ 1,
ziformula = ~ 1,
data = simulation
)
summary(betabin_model4)
anova(betabin_model4, betabin_model3)
betabin_model5 <- glmmTMB::glmmTMB(
cbind(papers_found, n_trials - papers_found) ~
condition +
n_abstracts_llm +
(1|dataset) + (0 + condition|dataset),
family = glmmTMB::betabinomial(link = "logit"),
dispformula = ~ 1,
ziformula = ~ 1,
data = simulation
)
summary(betabin_model5)
anova(betabin_model5, betabin_model4)
performance::r2_nakagawa(betabin_model5)
#return the datasets with 2 or less unique runs in 3 or more conditions
problematic_datasets <- unique_value_table %>%
group_by(dataset) %>%
summarise(
n_conditions_with_few_unique_values = sum(n_unique_values <= 2),
.groups = "drop"
) %>%
filter(n_conditions_with_few_unique_values >= 3)
problematic_datasets
#return the datasets with 2 or less unique runs in 3 or more conditions
problematic_datasets <- unique_value_table %>%
group_by(dataset) %>%
summarise(
n_conditions_with_few_unique_values = sum(n_unique_values <= 2),
.groups = "drop"
)
problematic_datasets
#return the rows from simulation corresponding to these dataset-condition combinations
problematic_combinations <- unique_value_table %>%
filter(n_unique_values <= 2) %>%
select(dataset, condition)
problematic_combinations
problematic_combinations <- merge(simulation, problematic_combinations, by = c("dataset", "condition"))
problematic_combinations
problematic_combinations
problematic_combinations %>%
filter(dataset == "Radjenovic_2013")
fraction_found <- simulation %>%
mutate(fraction_found = papers_found / n_trials)
fraction_found
fraction_found %>%
filter(fraction_found < 0)
fraction_found %>%
filter(fraction_found < 0)
p <- ggplot(simulation, aes(x = papers_found / n_trials, fill = condition)) +
geom_histogram(position = "dodge", bins = 30, alpha = 0.7) +
labs(
title = "Distribution of number of relevant records found by condition",
x = "Relevant records found (X) divided by number of trials (n <=100)",
y = "Count"
) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, face = "bold"),
panel.background = element_rect(fill = "white", color = NA),
plot.background  = element_rect(fill = "white", color = NA),
legend.background = element_rect(fill = "white", color = NA),
legend.key = element_rect(fill = "white", color = NA),
plot.margin = margin(10, 20, 10, 10)
)
# Show in R *with* title
p
# Export *without* title
ggsave(
filename = here::here("Report/results/papers_found_histogram.png"),
plot = p + labs(title = NULL),     # or + theme(plot.title = element_blank())
width = 10,
height = 6,
dpi = 300
)
